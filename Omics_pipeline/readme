#update		20151123	fangchao@genomics.cn
#This pipeline is to ease my pressure for Multiple omics analysis. For this version, I'm focus on the process of data polish.

# What do I wanna perform?
As a pipeline, I plan to orgainze the workshop directory like this:

./                 #output directory
|-- pip.work.sh		# A script contained all function set to run. A	MAIN SWITCH.
|-- Shell			# A directory contains all of scripts organized under steps order.
|-- Results			# A directory contains all of the results (exactly, including the intermediate results)

And all the user need to do is prepare a config file and write them into a script to build the workframe above.
#e.g /ifs1/ST_MD/PMO/F13ZOOYJSY1389_T2D_MultiOmics/Users/fangchao/lipidomics.20151118/pip.config.sh

#For a better understanding of the pipeline's logic, I'm about drawing a tree to show you how the pip.work.sh works:

./pip.work.sh
	|--> sh step1.sh
			|--> sh function1.sh
					|--> sh/perl sub-function scripts/software/program [parameters]
			|--> sh function2.sh
					|--> sh/perl sub-function scripts/software/program [parameters]
			...

As you can see, the sub-funtion tools may come from websites, package, or wirtten by yourself. And what you need to do is to locate the scripts pathway and make sure the parameters are friendly for most of the naming manners, such as capable to read and locate an absolute path. Thus you can leave the other things to the pipeline.

In the following step, I'll add your scripts into pipeline and distribute the unified input parameters as well as a proper output directory. Or some addtional options for the function of your part.
